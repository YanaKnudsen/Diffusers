{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanaKnudsen/Diffusers/blob/main/diffusers1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpffltAyvLxf"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/huggingface/diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7IeFejY-AjV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mG_TL7yWxEjr"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m65dT0TMvYe2"
      },
      "outputs": [],
      "source": [
        "pip install xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl_iPnclvbZ4"
      },
      "outputs": [],
      "source": [
        "pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX3OqmM6fqZs"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW-s7FIfeX-b"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw0FlAsbwuoM"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/prompthero/openjourney"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT09-gGX9Wsj"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/CompVis/stable-diffusion-v1-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQjt1Vp-3dx5"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import argparse\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging\n",
        "from tensorboardX import *\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "logging.basicConfig(format=\"%(asctime)s -%(levelname)s: %(message)s\",level=logging.INFO,datefmt=\"I%:%M:%S\")\n",
        "\n",
        "\n",
        "\n",
        "#Code diffusing tools, setting up noise schedualer, function for noise, sampling images\n",
        "class Diffusion:\n",
        "   def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02,img_size=64, device=\"cuda\"):\n",
        "       # parameters as in first paper\n",
        "       self.noise_steps=noise_steps\n",
        "       self.beta_start=beta_start\n",
        "       self.beta_end=beta_end\n",
        "       self.img_size=img_size\n",
        "       self.device=device\n",
        "\n",
        "       #linear beta schedule\n",
        "       self.beta=self.prepare_noise_schedule().to(device)\n",
        "       self.alpha=1.- self.beta\n",
        "       self.alpha_hat = torch.cumprod(self.alpha, dim = 0)\n",
        "\n",
        "   #linear beta schedule as in the first paper\n",
        "   def  prepare_noise_schedule(self):\n",
        "       return torch.linspace(self.beta_start,self.beta_end,self.noise_steps)\n",
        "\n",
        "   #function to introduce noise to the images\n",
        "   def noise_images(self,x,t):\n",
        "       #iteratively add noise over and over until at the desired step\n",
        "       sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:,None,None,None]\n",
        "       sqrt_one_minus_alpha_hat=torch.sqrt(1. - self.alpha_hat[t])[:,None,None,None]\n",
        "       e=torch.randn_like(x)\n",
        "       return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * e, e\n",
        "\n",
        "   #needed for the training (algorithm 1)\n",
        "   def  sample_timesteps(self,n):\n",
        "       return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
        "\n",
        "   #sampling function, takes model and number of images we would like to sample\n",
        "   def sample(self, model, n):\n",
        "       logging.info(\"Sampling {n} new images...\")\n",
        "       model.eval()\n",
        "       with torch.no_grad():\n",
        "            x=torch.randn((n,3,self.img_size,self.img_size)).to(self.device)\n",
        "            for i in tqdm(reversed(range(1,self.noise_steps)),position=0):\n",
        "\n",
        "                t=(torch.ones(n)*i).long().to(self.device)\n",
        "                predicted_noise=model(x,t)\n",
        "                alpha=self.alpha[t][:,None,None,None]\n",
        "                alpha_hat=self.alpha_hat[t][:,None,None,None]\n",
        "                beta=self.beta[t][:,None,None,None]\n",
        "                if i>1:\n",
        "                     noise=torch.randn_like(x)\n",
        "                else:\n",
        "\n",
        "                     noise=torch.zeros_like(x)\n",
        "\n",
        "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
        "\n",
        "\n",
        "       model.train()\n",
        "       x=(x.clamp(-1,1)+1)/2\n",
        "       x=(x*255).type(torch.uint8)\n",
        "       return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "   def __init__(self,c_in=3,c_out=3,time_dim=256,device=\"cuda\"):\n",
        "       super().__init__()\n",
        "       self.device=device\n",
        "       self.time_dim=time_dim\n",
        "       self.inc=DoubleConv(c_in,64)\n",
        "       self.down1=Down(64,128)\n",
        "       self.sa1=SelfAttention(128,32)\n",
        "       self.down2 =Down(128,256)\n",
        "       self.sa2=SelfAttention(256,16)\n",
        "       self.down3=Down(256,256)\n",
        "       self.sa3=SelfAttention(256,8)\n",
        "\n",
        "\n",
        "       self.bot1=DoubleConv(256,512)\n",
        "       self.bot2=DoubleConv(512,512)\n",
        "       self.bot3=DoubleConv(512,256)\n",
        "\n",
        "       self.up1=Up(512,128)\n",
        "       self.sa4=SelfAttention(128,16)\n",
        "       self.up2=Up(256,64)\n",
        "       self.sa5=SelfAttention(64,32)\n",
        "       self.up3=Up(128,64)\n",
        "       self.sa6=SelfAttention(64,64)\n",
        "       self.outc=nn.Conv2d(64,c_out,kernel_size=1)\n",
        "\n",
        "   def pos_encoding(self,t,channels):\n",
        "       inv_freq=1.0 /(\n",
        "           10000\n",
        "           ** (torch.arange(0,channels,2,device=self.device).float()/channels)\n",
        "       )\n",
        "       pos_enc_a=torch.sin(t.repeat(1,channels//2)*inv_freq)\n",
        "       pos_enc_b=torch.cos(t.repeat(1,channels//2)*inv_freq)\n",
        "       pos_enc=torch.cat([pos_enc_a,pos_enc_b],dim=-1)\n",
        "       return pos_enc\n",
        "\n",
        "   def forward(self,x,t):\n",
        "       t=t.unsqueeze(-1).type(torch.float)\n",
        "       t= self.pos_encoding(t, self.time_dim)\n",
        "\n",
        "       x1=self.inc(x)\n",
        "       x2=self.down1(x1,t)\n",
        "       x2=self.sa1(x2)\n",
        "       x3=self.down2(x2,t)\n",
        "       x3=self.sa2(x3)\n",
        "       x4=self.down3(x3,t)\n",
        "       x4= self.sa3(x4)\n",
        "\n",
        "       x4=self.bot1(x4)\n",
        "       x4=self.bot2(x4)\n",
        "       x4=self.bot3(x4)\n",
        "\n",
        "       x=self.up1(x4,x3,t)\n",
        "       x=self.sa4(x)\n",
        "       x=self.up2(x,x2,t)\n",
        "       x=self.sa5(x)\n",
        "       x=self.up3(x,x1,t)\n",
        "       x=self.sa6(x)\n",
        "       output=self.outc(x)\n",
        "       return output\n",
        "\n",
        "#normal convolution block: 2d convolution followed by a group norm and gelu activation\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self,in_channels, out_channels, mid_channels=None,residual=False):\n",
        "      super().__init__()\n",
        "      self.residual=residual\n",
        "      if not mid_channels:\n",
        "        mid_channels=out_channels\n",
        "      self.double_conv=nn.Sequential(\n",
        "          nn.Conv2d(in_channels,mid_channels,kernel_size=3,padding=1,bias=False),\n",
        "          nn.GroupNorm(1,mid_channels),\n",
        "          nn.GELU(),\n",
        "          nn.Conv2d(mid_channels,out_channels,kernel_size=3,padding=1,bias=False),\n",
        "          nn.GroupNorm(1,out_channels),\n",
        "      )\n",
        "\n",
        "    def forward(self,x):\n",
        "      if self.residual:\n",
        "        return F.gelu(x+self.double_conv(x))\n",
        "      else:\n",
        "        return self.double_conv(x)\n",
        "\n",
        "#max pool to reduce size by half followed by two double convs, important:time embedding layer\n",
        "class Down(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,emb_dim=256):\n",
        "      super().__init__()\n",
        "      self.maxpool_conv=nn.Sequential(\n",
        "          nn.MaxPool2d(2),\n",
        "          DoubleConv(in_channels,in_channels,residual=True),\n",
        "          DoubleConv(in_channels, out_channels),\n",
        "      )\n",
        "\n",
        "      self.emb_layer=nn.Sequential(\n",
        "          nn.SiLU(),\n",
        "          nn.Linear(\n",
        "              emb_dim,\n",
        "              out_channels\n",
        "          ),\n",
        "      )\n",
        "\n",
        "    def forward(self,x,t):\n",
        "      x=self.maxpool_conv(x)\n",
        "      emb=self.emb_layer(t)[:,:,None,None].repeat(1,1,x.shape[-2],x.shape[-1])\n",
        "      return x+emb\n",
        "\n",
        "#the same as down but upsampling instead maxpooling\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        self.conv = nn.Sequential(\n",
        "            DoubleConv(in_channels, in_channels, residual=True),\n",
        "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
        "        )\n",
        "\n",
        "        self.emb_layer = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_dim,\n",
        "                out_channels\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_x, t):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([skip_x, x], dim=1)\n",
        "        x = self.conv(x)\n",
        "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
        "        return x + emb\n",
        "\n",
        "#self attention layer\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self,channels,size):\n",
        "      super(SelfAttention,self).__init__()\n",
        "      self.channels=channels\n",
        "      self.size=size\n",
        "      self.mha=nn.MultiheadAttention(channels,4,batch_first=True)\n",
        "      self.ln=nn.LayerNorm([channels])\n",
        "      self.ff_self=nn.Sequential(\n",
        "          nn.LayerNorm([channels]),\n",
        "          nn.Linear(channels,channels),\n",
        "          nn.GELU(),\n",
        "          nn.Linear(channels,channels),\n",
        "      )\n",
        "    def forward(self,x):\n",
        "      x=x.view(-1,self.channels,self.size*self.size).swapaxes(1,2)\n",
        "      x_ln=self.ln(x)\n",
        "      attention_value,_=self.mha(x_ln,x_ln,x_ln)\n",
        "      attention_value=attention_value+x\n",
        "      attention_value=self.ff_self(attention_value)+attention_value\n",
        "      return attention_value.swapaxes(2,1).view(-1,self.channels,self.size,self.size)\n",
        "\n",
        "\n",
        "#useful functions\n",
        "#potting images\n",
        "def plot_images(images):\n",
        "  plt.figure(figsize=(32,32))\n",
        "  plt.imshow(torch.cat([\n",
        "      torch.cat([i for i in images.cpu()],dim=-1)\n",
        "  ],dim=-2).permute(1,2,0).cpu())\n",
        "  plt.show()\n",
        "\n",
        "#saving images\n",
        "def save_images(images,path,**kwargs):\n",
        "  grid=torchvision.utils.make_grid(images,**kwargs)\n",
        "  ndarr=grid.permute(1,2,0).to(\"cpu\").numpy()\n",
        "  im=Image.fromarray(ndarr)\n",
        "  im.save(path)\n",
        "\n",
        "#preparing data\n",
        "def get_data(args):\n",
        "  transforms=torchvision.transforms.Compose([\n",
        "      torchvision.transforms.Resize(80),\n",
        "      torchvision.transforms.RandomResizedCrop(args.img_size,scale=(0.8,1.0)),\n",
        "      torchvision.transforms.ToTensor(),\n",
        "      torchvision.transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "  ])\n",
        "  #define dataset\n",
        "  dataset=torchvision.datasets.ImageFolder(args.dataset_path, transform=transforms)\n",
        "  dataloader=DataLoader(dataset,batch_size=args.batch_size,shuffle=True)\n",
        "  return dataloader\n",
        "\n",
        "#saving model and results\n",
        "def setup_logging(run_name):\n",
        "   os.makedirs(\"/content/drive/MyDrive/landscapes/models\",exist_ok=True)\n",
        "   os.makedirs(\"/content/drive/MyDrive/landscapes/results\",exist_ok=True)\n",
        "   os.makedirs(\"/content/drive/MyDrive/landscapes/mymodels\",exist_ok=True)\n",
        "   os.makedirs(os.path.join(\"/content/drive/MyDrive/landscapes/models\",run_name),exist_ok=True)\n",
        "   os.makedirs(os.path.join(\"/content/drive/MyDrive/landscapes/results\",run_name),exist_ok=True)\n",
        "   os.makedirs(os.path.join(\"/content/drive/MyDrive/landscapes/mymodels\",run_name),exist_ok=True)\n",
        "\n",
        "\n",
        "#training\n",
        "def train(args):\n",
        "  setup_logging(args.run_name)\n",
        "  device=args.device\n",
        "  dataloader=get_data(args)\n",
        "  model=UNet().to(device)\n",
        "  optimizer=optim.AdamW(model.parameters(),lr=args.lr)\n",
        "  #load model here\n",
        "  checkpoint = torch.load(os.path.join(\"/content/drive/MyDrive/landscapes/mymodels\",args.run_name,\"model.pt\"))\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "\n",
        "  mse=nn.MSELoss()\n",
        "  diffusion=Diffusion(img_size=args.img_size,device=device)\n",
        "  logger=SummaryWriter(os.path.join(\"runs\",args.run_name))\n",
        "  l=len(dataloader)\n",
        "\n",
        "  for epoch in range(epoch,args.epoch):\n",
        "\n",
        "    print(\"Epoch:\",epoch)\n",
        "    logging.info(\"Starting epoch {epoch}:\")\n",
        "    pbar=tqdm(dataloader)\n",
        "    for i,(images, _) in enumerate(pbar):\n",
        "      images=images.to(device)\n",
        "      t=diffusion.sample_timesteps(images.shape[0]).to(device)\n",
        "      x_t,noise=diffusion.noise_images(images,t)\n",
        "      predicted_noise=model(x_t,t)\n",
        "      loss=mse(noise,predicted_noise)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      pbar.set_postfix(MSE=loss.item())\n",
        "      logger.add_scalar(\"MSE\",loss.item(),global_step=epoch*l+i)\n",
        "\n",
        "    sampled_images=diffusion.sample(model,n=images.shape[0])\n",
        "    save_images(sampled_images,os.path.join(\"/content/drive/MyDrive/landscapes/results\",args.run_name,str(epoch)+\".jpg\"))\n",
        "    torch.save(model.state_dict(),os.path.join(\"/content/drive/MyDrive/landscapes/models\",args.run_name,\"ckpt.pt\"))\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': mse,\n",
        "            }, os.path.join(\"/content/drive/MyDrive/landscapes/mymodels\",args.run_name,\"model.pt\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "parser=argparse.ArgumentParser()\n",
        "args=parser.parse_args(\"\")\n",
        "args.run_name=\"DDPM_Unconditional\"\n",
        "args.epoch=500\n",
        "args.batch_size=10\n",
        "args.img_size=64\n",
        "args.dataset_path=r\"/content/drive/MyDrive/landscapes/train/\"\n",
        "args.device=\"cuda\"\n",
        "args.lr=3e-4\n",
        "train(args)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to do https://www.youtube.com/watch?v=TBCRlnwJtZU&t=272s  outlier"
      ],
      "metadata": {
        "id": "SihjlQv1ywlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQCB4r9ihHcM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP0xK0NV44Wvv2OMoAF8Sk4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}